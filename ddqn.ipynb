{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Define a named tuple that represents an experience (transition)\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyper parameters for the DDQN algorithm\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "# The initial porbability of uniformly selecting an action (not greedy)\n",
    "EPS_START = 0.9\n",
    "# The final probability of uniformly selecting an action (not greedy)\n",
    "EPS_END = 0.05\n",
    "# The decay size on the probability of uniformly selecting an action\n",
    "EPS_DECAY = 1000\n",
    "# The extent to which the target network parameters should be updated (taken from policy network)\n",
    "TAU = 0.005\n",
    "# Learning rate for optimization\n",
    "LR = 1e-4\n",
    "# Size of the memory\n",
    "MEMORY_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cart-pole game\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Number of actions possible from each state\n",
    "n_actions = env.action_space.n\n",
    "# Number of dimensions (representation) for each state\n",
    "state, info = env.reset()\n",
    "dim_state = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class that represents the experience replay memory\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args) -> None:\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        # Sampling uniformly at random (without replacement)\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)\n",
    "\n",
    "# Define the policy as well as the target network\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    '''\n",
    "    A fully connected neural network representing the policy (Q)\n",
    "    and the target networks\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    dim_state: The dimensionality of the states\n",
    "    n_actions: The number of possible actions from each state\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim_state: int, n_actions: int) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(dim_state, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "def select_action(state: torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Performs epsilon-greedy action selection given a state\n",
    "    '''\n",
    "    # The steps_done is in the global scope\n",
    "    global steps_done\n",
    "    # Generate randomly a number between 0 and 1 \n",
    "    sample = random.random()\n",
    "    # Probability for the epsilon-greedy selection\n",
    "    # It decays as the number of steps grow\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    # Returns a tensor of dim (1, 1) (tensor([[1]]), for example)\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) (t stands for a tensor) returns a tensor\n",
    "            # where the first column is the largest column found for \n",
    "            # each row in t and the second column is the index\n",
    "            # of the column at which the maximum value happened. \n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model():\n",
    "    '''\n",
    "    Double DQN (DDQN) implementation (Hasselt et al, 2015)\n",
    "    '''\n",
    "    # No update if the buffer is not adequately populated\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    # A boolean tensor (true if the next state is not None and false, otherwise)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # Concat only next states that are not None (with a non zero Q)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                 if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Get the action values (Q) for each state and the action chosen\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        # Get the action for the next state whose expected value\n",
    "        # evaluated by the policy network is maximum (argmax Q(next_state, action; θ))\n",
    "        next_state_actions = policy_net(non_final_next_states).max(1).indices.view(-1, 1)\n",
    "        # Get the value of the action (selected above) using\n",
    "        # the target network (Q(next_state, action; θ'))\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states) \\\n",
    "            .gather(1, next_state_actions).squeeze(1)\n",
    "    \n",
    "    # Get the target values for the policy network (needed for optimization)\n",
    "    expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    # Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    # unsqueeze adds dim 1 into the requested axis (here, the tensor is reshaped into a column vector)\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Apply the gradient clipping\n",
    "    nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    # Update the parameters of the policy network\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy and the target networks\n",
    "policy_net = DQN(dim_state, n_actions).to(device)\n",
    "# Targets for the policy net are provided by the target net\n",
    "target_net = DQN(dim_state, n_actions).to(device)\n",
    "# Copy the parameters from the policy net into the target net\n",
    "target_net.load_state_dict(policy_net.state_dict()) \n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "# Buffer using which the experiences are stored and\n",
    "# sampled (uniformly at random) later\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "# Number of steps (t) taken so far\n",
    "steps_done = 0\n",
    "\n",
    "# Number of episodes (playing the game)\n",
    "num_episodes = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        # Get the action using the current policy network\n",
    "        action = select_action(state)\n",
    "        # Get the next state, reward, status of the next state, ...\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "        # If the next state is a terminal state, then set it to None\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            # A row vector of shape (1, 4)\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Add the experience to the buffer \n",
    "        memory.push(state, action, next_state, reward)\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        # Sample from the buffer and optimize the policy network parameters  \n",
    "        optimize_model()\n",
    "        # Copy the parameters of the policy network to the target network\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        # Update the parameters of the target network\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        if done:\n",
    "            # End of the episode\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
