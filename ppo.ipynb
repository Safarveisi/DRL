{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "from collections import namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# Standard deviation of the normal distribution\n",
    "STD = 0.5\n",
    "# Learning rate for the gradient ascent/descent\n",
    "LR = 0.005\n",
    "# Doscount factor\n",
    "GAMMA = 0.95\n",
    "# CLIP interval (1 - EPSILON, 1 + EPSILON)\n",
    "EPSILON = 0.2\n",
    "# Number of times to repeat learning using a batch\n",
    "EPOCHS = 5\n",
    "# A named tuple that represents the transitions\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'log_prob', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy network is an estimator for the mean of a normal distribution\n",
    "class FeedForwardNN(nn.Module):\n",
    "\t'''\n",
    "\t\tA standard in_dim-64-64-out_dim Feed Forward Neural Network.\n",
    "\t'''\n",
    "\tdef __init__(self, in_dim: int, out_dim: int) -> None:\n",
    "\t\tsuper(FeedForwardNN, self).__init__()\n",
    "\n",
    "\t\tself.layer1 = nn.Linear(in_dim, 64)\n",
    "\t\tself.layer2 = nn.Linear(64, 64)\n",
    "\t\tself.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "\tdef forward(self, obs: torch.Tensor):\n",
    "\n",
    "\t\tactivation1 = F.relu(self.layer1(obs))\n",
    "\t\tactivation2 = F.relu(self.layer2(activation1))\n",
    "\t\toutput = self.layer3(activation2)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(env.observation_space) == gym.spaces.Box\n",
    "assert type(env.action_space) == gym.spaces.Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of the state representation\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "# The dimensionality of the action representation should be 1\n",
    "assert env.action_space.shape[0] == 1\n",
    "act_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actor network\n",
    "actor = FeedForwardNN(obs_dim, act_dim).to(device)\n",
    "# The critic network\n",
    "critic = FeedForwardNN(obs_dim, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor and critic networks optimizer\n",
    "actor_optim = AdamW(actor.parameters(), lr=LR, amsgrad=True)\n",
    "critic_optim = AdamW(critic.parameters(), lr=LR, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''Get the action (sampled) from the Normal distribution\n",
    "       generated by the given state\n",
    "    '''\n",
    "    # The mean for the actor gaussian distribution\n",
    "    mean = actor(obs)\n",
    "    # actor ~ Normal(mean, std=0.5)\n",
    "    dist = Normal(loc=mean, scale=0.5)\n",
    "    # Sample an action from the current normal distribution\n",
    "    action = dist.sample()\n",
    "    # Get the log probability of the action sampled \n",
    "    log_prob = dist.log_prob(action)\n",
    "    # Detach log_prob as it depends on the paramaters of the actor \n",
    "    return action, log_prob.detach()\n",
    "    \n",
    "def generate_batch() -> Tuple[torch.Tensor,...]:\n",
    "    '''Generates a batch of atomic experiences from running different\n",
    "       episodes. It will be called several times depending on the \n",
    "       number of learning iterations.   \n",
    "    '''\n",
    "    \n",
    "    # All transitions in a batch\n",
    "    transitions = []\n",
    "    # Maximum timesteps per batch (may be violated to some extent)\n",
    "    MAX_TIMESTEPS_PER_BATCH = 4800\n",
    "    # Maximum timesteps per episode\n",
    "    MAX_TIMESTEPS_PER_EPISODE = 1600\n",
    "    # Initial timestep in the batch\n",
    "    t = 0\n",
    "\n",
    "    # Collect transitions for the batch\n",
    "    while t < MAX_TIMESTEPS_PER_BATCH:\n",
    "        # Get the first state in the episode\n",
    "        state, info = env.reset()\n",
    "        # Convert it into a torch tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Run until we reach the max timestep for an episode or a terminal state\n",
    "        for _ in range(MAX_TIMESTEPS_PER_EPISODE):\n",
    "            # Increment the timestemp (t0 -> t1 -> t2 ...)\n",
    "            t += 1\n",
    "            # Get the action using the current actor network\n",
    "            action, log_prob = get_action(state)\n",
    "            # Get the next state, reward, next state and its status (if it is a terminal)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.squeeze(0).detach().numpy())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            # Was the next state a terminal state?\n",
    "            done = terminated or truncated\n",
    "            # If the next state is a terminal state, then set it to None\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            # Add the experience to the buffer \n",
    "            transitions.append(Transition(state, action, log_prob, next_state, reward))\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break # End of episode\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = len(transitions)\n",
    "    # Convert the batch into a Transition object\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    # A boolean tensor to indicate if a state is terminal in the batch\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "    # initialize V(next_state) for all next_state in the batch\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    # V(next_state) = 0 for terminal next_state\n",
    "    next_state_values[non_final_mask] = critic(non_final_next_states).squeeze(1)\n",
    "    # Batch states\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    # Batch actions (in each state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    # Batch log probs\n",
    "    log_prob_batch = torch.cat(batch.log_prob)\n",
    "    # Batch rewards (as a result of actions in each state)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Get the target for this batch (r + \\gmma * V(next_state) - V(state))\n",
    "    # The target can also be used as an estimator of A_t\n",
    "    target = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    return state_batch, action_batch, log_prob_batch, target.detach()\n",
    "\n",
    "def learn(total_timesteps: int) -> None:\n",
    "    '''Learns the critic and actor\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    total_timesteps: The maximum number of timesteps throughout the\n",
    "                     entire learning process. \n",
    "    '''\n",
    "    # The initial timestep throughout the entire learning process\n",
    "    t = 0\n",
    "    \n",
    "    while t < total_timesteps:\n",
    "        # Generate a batch such that we can iterate through later\n",
    "        batch = generate_batch()\n",
    "        # Add the number of timesteps generated in the batch to the initial t\n",
    "        t += len(batch)\n",
    "        # A boolean tensor to indicate if a state is terminal in the batch\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "        # Batch states\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        # Batch actions (in each state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        # Batch log probs\n",
    "        log_prob_batch = torch.cat(batch.log_prob)\n",
    "        # Batch rewards (as a result of actions in each state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    V = critic(batch_obs).squeeze()\n",
    "    mean = actor(batch_obs)\n",
    "    dist = Normal(loc=mean, scale=0.5)\n",
    "    # New log values for the actions in the batch\n",
    "    log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "    return V, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
