{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Optional, List\n",
    "from collections import namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')\n",
    "# Standard deviation of the normal distribution\n",
    "STD = 0.5\n",
    "# Learning rate for the gradient ascent/descent\n",
    "LR = 0.005\n",
    "# Doscount factor\n",
    "GAMMA = 0.95\n",
    "# CLIP interval (1 - EPSILON, 1 + EPSILON)\n",
    "EPSILON = 0.2\n",
    "# Number of times to repeat learning using a batch\n",
    "EPOCHS = 5\n",
    "# Maximum timesteps per batch (may be violated to some extent)\n",
    "MAX_TIMESTEPS_PER_BATCH = 4800\n",
    "# Maximum timesteps per episode\n",
    "MAX_TIMESTEPS_PER_EPISODE = 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A named tuple that represents the transitions\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'log_prob', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy network is an estimator for the mean of a normal distribution\n",
    "class FeedForwardNN(nn.Module):\n",
    "\t'''\n",
    "\t\tA Feed Forward Neural Network of the following structure:\n",
    "\t\t\t\t\t\tin_dim-64-64-out_dim\n",
    "\t'''\n",
    "\tdef __init__(self, in_dim: int, out_dim: int) -> None:\n",
    "\t\tsuper(FeedForwardNN, self).__init__()\n",
    "\n",
    "\t\tself.layer1 = nn.Linear(in_dim, 64)\n",
    "\t\tself.layer2 = nn.Linear(64, 64)\n",
    "\t\tself.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor):\n",
    "\n",
    "\t\tactivation1 = F.relu(self.layer1(x))\n",
    "\t\tactivation2 = F.relu(self.layer2(activation1))\n",
    "\t\toutput = self.layer3(activation2)\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an environment with a continuous action space\n",
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the action and state spaces are of type BOX\n",
    "assert type(env.observation_space) == gym.spaces.Box\n",
    "assert type(env.action_space) == gym.spaces.Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimensionality of the state representation\n",
    "dim_state = env.observation_space.shape[0]\n",
    "# The dimensionality of the action representation should be 1\n",
    "assert env.action_space.shape[0] == 1\n",
    "# With the test passed, set the action dimensionality to 1\n",
    "dim_action = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actor network\n",
    "actor = FeedForwardNN(dim_state, dim_action).to(DEVICE)\n",
    "# The critic network\n",
    "critic = FeedForwardNN(dim_state, 1).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer for actor and critic networks \n",
    "actor_optim = AdamW(actor.parameters(), lr=LR, amsgrad=True)\n",
    "critic_optim = AdamW(critic.parameters(), lr=LR, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''Get the action (sampled) from the Normal distribution\n",
    "       generated by the given state\n",
    "    '''\n",
    "    # The mean for the actor gaussian distribution\n",
    "    mean = actor(obs)\n",
    "    # actor ~ Normal(mean, std=0.5)\n",
    "    dist = Normal(loc=mean, scale=0.5)\n",
    "    # Sample an action from the current normal distribution\n",
    "    action = dist.sample()\n",
    "    # Get the log probability of the action sampled \n",
    "    log_prob = dist.log_prob(action)\n",
    "    # Detach log_prob as it depends on the parameters of the actor \n",
    "    return action, log_prob.detach()\n",
    "\n",
    "def evaluate(\n",
    "    batch_state: torch.Tensor,\n",
    "    batch_action: torch.Tensor,\n",
    "    only_v: bool = False\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "    '''Computes the state value for each state\n",
    "       in the batch. If only_v is False, it will\n",
    "       also computes the log probabilities of the\n",
    "       actions sampled from the generated gaussian \n",
    "       distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    batch_state:  Tensor of states in the batch\n",
    "    batch_action: Tensor of actions taken for each state in the batch\n",
    "    only_v:       Whether the log probabilities of the actions in the batch\n",
    "                  is needed. \n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    A tuple of two tensors (with the second element optional)\n",
    "    '''\n",
    "    # Will remain None if only_v is True\n",
    "    log_probs = None\n",
    "    V = critic(batch_state).squeeze(1)\n",
    "    \n",
    "    if not only_v:\n",
    "        mean = actor(batch_state)\n",
    "        dist = Normal(loc=mean, scale=0.5)\n",
    "        log_probs = dist.log_prob(batch_action)\n",
    "    \n",
    "    return V, log_probs\n",
    "\n",
    "def generate_batch() -> List[Transition]:\n",
    "    '''Generates a batch of atomic experiences from running different\n",
    "       episodes. It will be called several times depending on the \n",
    "       number of learning iterations.   \n",
    "    '''\n",
    "    \n",
    "    # All transitions in a batch\n",
    "    batch_transitions = []\n",
    "    # Initial timestep in the batch\n",
    "    t = 0\n",
    "\n",
    "    # Collect transitions for the batch\n",
    "    while t < MAX_TIMESTEPS_PER_BATCH:\n",
    "        # Get the first state in the episode\n",
    "        state, info = env.reset()\n",
    "        # Convert it into a torch tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        # Run until we reach the max timestep for an episode or a terminal state\n",
    "        for _ in range(MAX_TIMESTEPS_PER_EPISODE):\n",
    "            # Increment the timestemp (t0 -> t1 -> t2 ...)\n",
    "            t += 1\n",
    "            # Get the action using the current actor network\n",
    "            action, log_prob = get_action(state)\n",
    "            # Get the next state, reward, and the kind of the next state (if it is a terminal)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.squeeze(0).detach().numpy())\n",
    "            reward = torch.tensor([reward], device=DEVICE)\n",
    "            # Was the next state a terminal state?\n",
    "            done = terminated or truncated\n",
    "            # If the next state is a terminal state, then set it to None\n",
    "            if terminated:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = torch.tensor(observation, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "            # Add the experience to the bactch \n",
    "            batch_transitions.append(Transition(state, action, log_prob, next_state, reward))\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break # End of the episode\n",
    "\n",
    "    return batch_transitions\n",
    "\n",
    "def decompose(batch_transitions: List[Transition]) -> Tuple[torch.Tensor,...]:\n",
    "    '''Extract states, actions taken in each state,\n",
    "       received rewards, next state, targets, and\n",
    "       estimations of the advantage function as a\n",
    "       torch tensor.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    batch_transitions: A list of all transitions in a batch \n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    A tuple of torch tensors\n",
    "    '''\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = len(batch_transitions)\n",
    "    \n",
    "    # Convert the batch into a Transition object\n",
    "    batch = Transition(*zip(*batch_transitions))\n",
    "    \n",
    "    # A boolean tensor to indicate if a state is terminal in the batch\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), device=DEVICE, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "    # initialize V(next_state) for all next_state in the batch\n",
    "    next_state_values = torch.zeros(batch_size, device=DEVICE)\n",
    "    \n",
    "    # V(next_state) = 0 for terminal next_state\n",
    "    next_state_values[non_final_mask] = critic(non_final_next_states).squeeze(1)\n",
    "    \n",
    "    # Batch states\n",
    "    batch_state = torch.cat(batch.state)\n",
    "    \n",
    "    # Batch actions (in each state)\n",
    "    batch_action = torch.cat(batch.action)\n",
    "    \n",
    "    # Batch log probs\n",
    "    batch_log_prob = torch.cat(batch.log_prob)\n",
    "    \n",
    "    # Batch rewards (as a result of actions in each state)\n",
    "    batch_reward = torch.cat(batch.reward)\n",
    "    \n",
    "    # Get the target for this batch (r + GAMMA * V(next_state))\n",
    "    batch_target = (batch_reward + GAMMA * next_state_values).detach()\n",
    "    \n",
    "    return batch_state, batch_action, batch_log_prob, batch_target\n",
    "\n",
    "def learn(total_timesteps: int) -> None:\n",
    "    '''Learns the critic and actor\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    total_timesteps: The maximum number of timesteps throughout the\n",
    "                     entire learning process. \n",
    "    '''\n",
    "    # The initial timestep throughout the entire learning process\n",
    "    t = 0\n",
    "\n",
    "    # Number of batches generated so far\n",
    "    iter = 0\n",
    "    \n",
    "    while t < total_timesteps:\n",
    "        \n",
    "        # Generate a batch such that we can iterate through later\n",
    "        batch_state, batch_action, batch_log_prob, batch_target = decompose(batch_transitions := generate_batch())\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        print(f'Iteration {iter} started ...')\n",
    "        # Add the number of timesteps generated in the batch to the initial t\n",
    "        t += len(batch_transitions) \n",
    "         # Get the state value (V(state) for all states in the batch)\n",
    "        V, _ = evaluate(batch_state, batch_action, only_v=True) \n",
    "        # Normalized advantage function estimation\n",
    "        A_hat = batch_target - V.detach()\n",
    "        A_hat = (A_hat - A_hat.mean()) / (A_hat.std() + 1e-10)\n",
    "        \n",
    "        for i in range(1, EPOCHS + 1):\n",
    "            print(f'\\t>>> Epoch {i} started ...')\n",
    "            V, curr_log_prob = evaluate(batch_state, batch_action)\n",
    "            ratios = torch.exp(curr_log_prob - batch_log_prob)\n",
    "            surr1 = ratios * A_hat\n",
    "            surr2 = torch.clamp(ratios, 1 - EPSILON, 1 + EPSILON) * A_hat\n",
    "            # CLIP loss for actor\n",
    "            actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "            # MSE loss for critic\n",
    "            critic_loss = nn.MSELoss()(V, batch_target)\n",
    "\n",
    "            # Calculate gradients and perform backward propagation for actor\n",
    "            actor_optim.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "            # Calculate gradients and perform backward propagation for critic\n",
    "            critic_optim.zero_grad()\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_optim.step()\n",
    "        \n",
    "        print(f'Iteration {iter} finished!')\n",
    "        print('***************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
