{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import math\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    '''\n",
    "    A fully connected neural network representing the policy (Q)\n",
    "    and the target networks\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    dim_state: The dimensionality of the states\n",
    "    n_actions: The number of possible actions from each state\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim_state: int, n_actions: int) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(dim_state, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "# The initial porbability of uniformly selecting an action (not greedy)\n",
    "EPS_START = 0.9\n",
    "# The final probability of uniformly selecting an action (not greedy)\n",
    "EPS_END = 0.05\n",
    "# The decay size on the probability of uniformly selecting an action\n",
    "EPS_DECAY = 1000\n",
    "# The extent to which the target network parameters should be updated (taken from policy network)\n",
    "TAU = 0.005\n",
    "# Learning rate for optimization\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of actions possible from each state\n",
    "n_actions = env.action_space.n\n",
    "# Number of dimensions (representation) for each state\n",
    "state, info = env.reset()\n",
    "dim_state = len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net = DQN(dim_state, n_actions).to(device)\n",
    "# Targets for the policy net are provided by the target net\n",
    "target_net = DQN(dim_state, n_actions).to(device)\n",
    "# Copy the parameters from the policy net into the target net\n",
    "target_net.load_state_dict(policy_net.state_dict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Performs epsilon-greedy action selection given a state\n",
    "    '''\n",
    "    # The steps_done is in the global scope\n",
    "    global steps_done\n",
    "    # Generate randomly a number between 0 and 1 \n",
    "    sample = random.random()\n",
    "    # Probability for the epsilon-greedy selection\n",
    "    # It decays as the number of steps grow\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    # Returns a tensor of dim (1, 1) (tensor([[1]]), for example)\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) (t stands for a tensor) returns a tensor\n",
    "            # where the first column is the largest column found for \n",
    "            # each row in t and the second column is the index\n",
    "            # of the column at which the maximum value happened. \n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    '''Generic Node class. A node is a basic unit of a data structure.\n",
    "       Used in the implementation of a prioritized replay buffer.\n",
    "    '''\n",
    "    def __init__(self, key, value):\n",
    "        # Holds the TD error\n",
    "        self.key = key\n",
    "        # Holds the experience tuple\n",
    "        self.value = value\n",
    "    def update_key_and_value(self, new_key, new_value):\n",
    "        '''Updates the key and value at the same time'''\n",
    "        self.update_key(new_key)\n",
    "        self.update_value(new_value)\n",
    "    def update_key(self, new_key):\n",
    "        '''Assinging new TD error to the experience'''\n",
    "        self.key = new_key\n",
    "    def update_value(self, new_value):\n",
    "        '''Overwriting when the experience with a new one'''\n",
    "        self.value = new_value\n",
    "    def __eq__(self, other):\n",
    "        '''Two nodes are equal if an only if their keys and values are equal'''\n",
    "        return self.key == other.key and self.value == other.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxHeap(object):\n",
    "    \n",
    "    def __init__(self, max_size, dimension_of_value_attribute, default_key_to_use):\n",
    "        # Max size of the deque (max_size * 4 + 1, for the heap) \n",
    "        self.max_size = max_size\n",
    "        # Dimension of the tuple that represents an experience\n",
    "        self.dimension_of_value_attribute = dimension_of_value_attribute\n",
    "        # The default key for the nodes (during initialization)\n",
    "        self.default_key_to_use = default_key_to_use\n",
    "        # Initializes the heap with the attributes set above\n",
    "        self.heap = self.initialize_heap()\n",
    "    \n",
    "    def initialize_heap(self):\n",
    "        '''Initialize a heap of Nodes of length self.max_size * 4 + 1'''\n",
    "        heap = np.array([Node(self.default_key_to_use, tuple([None for _ in range(self.dimension_of_value_attribute)]))\n",
    "                         for _ in range(self.max_size * 4 + 1)])\n",
    "        # The first element in the heap cannot be swapped \n",
    "        heap[0] = Node(float('inf'), tuple([None for _ in range(self.dimension_of_value_attribute)]))\n",
    "        return heap\n",
    "    \n",
    "    def update_heap_element(self, heap_index, new_element):\n",
    "        '''Updates the node at heap_index'''\n",
    "        self.heap[heap_index] = new_element\n",
    "\n",
    "    def swap_heap_elements(self, index1, index2):\n",
    "        '''Swaps the position of two heap elements'''\n",
    "        self.heap[index1], self.heap[index2] = self.heap[index2], self.heap[index1]\n",
    "    \n",
    "    def calculate_index_of_biggest_child(self, heap_index_changed):\n",
    "        '''heap_index_changed is the index at which there was a change.\n",
    "        \n",
    "           Get the index of its biggest child using the following formula \n",
    "                (1) Left child's index = heap_index_changed * 2\n",
    "                (2) Right child's index = heap_index_changed * 2 + 1    \n",
    "        '''\n",
    "        left_child = self.heap[int(heap_index_changed * 2)]\n",
    "        right_child = self.heap[int(heap_index_changed * 2) + 1]\n",
    "        # Get the biggest child's index (in terms of the TD error)\n",
    "        if left_child.key > right_child.key:\n",
    "            return heap_index_changed * 2\n",
    "        else:\n",
    "            return heap_index_changed * 2 + 1\n",
    "    \n",
    "    # The element at position 1 has the highest key (TD error)\n",
    "    def give_max_key(self):\n",
    "        '''Get the maximum TD error from the heap'''\n",
    "        return self.heap[1].key\n",
    "\n",
    "    def reorganize_heap(self, heap_index_changed):\n",
    "        '''heap_index_changed is the index at which there was a change.\n",
    "           When there is a change at an index, we need to re-sort the array.\n",
    "           Calling the function with heap_index_changed = 1 results in no change.  \n",
    "        '''\n",
    "        # New TD error for the experience stored at heap_index_changed\n",
    "        node_key = self.heap[heap_index_changed].key\n",
    "        # Get the parent index of the heap_index_changed\n",
    "        parent_index = int(heap_index_changed / 2)\n",
    "        \n",
    "        if node_key > self.heap[parent_index].key:\n",
    "            # Change the element at heap_index_changed with the element at position parent_index   \n",
    "            self.swap_heap_elements(heap_index_changed, parent_index)\n",
    "            # Recall the function with parent_index (upward direction)\n",
    "            self.reorganize_heap(parent_index)\n",
    "        else:\n",
    "            # Get the index of the biggest child (in terms of the Td error)\n",
    "            biggest_child_index = self.calculate_index_of_biggest_child(heap_index_changed)\n",
    "            if node_key < self.heap[biggest_child_index].key:\n",
    "                # Change the element at heap_index_changed with the element at position biggest_child_index\n",
    "                self.swap_heap_elements(heap_index_changed, biggest_child_index)\n",
    "                # Recall the function with biggest_child_index (downward direction)\n",
    "                self.reorganize_heap(biggest_child_index)\n",
    "\n",
    "    def update_element_and_reorganize_heap(self, heap_index_changed, new_element):\n",
    "        '''Updates the element at the specified heap_index_changed with a new node.'''\n",
    "        self.update_heap_element(heap_index_changed, new_element)\n",
    "        self.reorganize_heap(heap_index_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deque(object):\n",
    "    '''Generic deque object'''\n",
    "    \n",
    "    def __init__(self, max_size, dimension_of_value_attribute, default_key_to_use):\n",
    "        # Maximum size of the queue \n",
    "        self.max_size = max_size\n",
    "        # Dimension of the value (a tuple) for the nodes in the queue\n",
    "        self.dimension_of_value_attribute = dimension_of_value_attribute\n",
    "        # Initialize the queue\n",
    "        self.deque = self.initialize_deque()\n",
    "        # The next index in the queue to be modified \n",
    "        self.deque_index_to_overwrite_next = 0\n",
    "        # If we are at the end of the queue\n",
    "        self.reached_max_capacity = False\n",
    "        self.number_experiences_in_deque = 0\n",
    "        # The default key for the nodes (during initialization)\n",
    "        self.default_key_to_use = default_key_to_use\n",
    "\n",
    "    def initialize_deque(self):\n",
    "        '''Initializes a queue of nodes of length self.max_size'''\n",
    "        deque = np.array([Node(self.default_key_to_use, tuple([None for _ in range(self.dimension_of_value_attribute)]))\n",
    "                          for _ in range(self.max_size)])\n",
    "        return deque\n",
    "    \n",
    "    def update_deque_node_key(self, index, new_key):\n",
    "        '''Updates a node's key at the specified index'''\n",
    "        self.deque[index].update_key(new_key)\n",
    "    \n",
    "    def update_deque_node_value(self, index, new_value):\n",
    "        '''Updates a node's value at the specified index'''\n",
    "        self.deque[index].update_value(new_value)\n",
    "    \n",
    "    def update_deque_node_key_and_value(self, index, new_key, new_value):\n",
    "        '''Updates a node's key and value at the specified index'''\n",
    "        self.update_deque_node_key(index, new_key)\n",
    "        self.update_deque_node_value(index, new_value)\n",
    "    \n",
    "    def update_deque_index_to_overwrite_next(self):\n",
    "        '''Increments the attribute deque_index_to_overwrite_next by one\n",
    "           if we have not reached the end of the queue; otherwise,\n",
    "           sets it equal to zero, indicating a return to the bottom.  \n",
    "        '''\n",
    "        # The index starts from 0 and ends at self.max_size - 1\n",
    "        if self.deque_index_to_overwrite_next < self.max_size - 1:\n",
    "            self.deque_index_to_overwrite_next += 1\n",
    "        else:\n",
    "            self.reached_max_capacity = True\n",
    "            # Go back to the bottom\n",
    "            self.deque_index_to_overwrite_next = 0\n",
    "    \n",
    "    def add_element_to_deque(self, new_key, new_value):\n",
    "        '''Adds a new element (a node) to the queue'''\n",
    "        self.update_deque_node_key_and_value(self.deque_index_to_overwrite_next, new_key, new_value)\n",
    "        self.update_number_experiences_in_deque()\n",
    "        self.update_deque_index_to_overwrite_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(MaxHeap, Deque):\n",
    "    \n",
    "    def __init__(self, hyperparameters, seed=0):\n",
    "        \n",
    "        MaxHeap.__init__(self, hyperparameters['buffer_size'], dimension_of_value_attribute=4, default_key_to_use=0)\n",
    "        Deque.__init__(self, hyperparameters['buffer_size'], dimension_of_value_attribute=4, default_key_to_use=0)\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.deques_td_errors = self.initialize_td_errors_array()\n",
    "        # Remember: index 0 in the heap cannot be swapped or modified\n",
    "        self.heap_index_to_overwrite_next = 1\n",
    "        self.number_experiences_in_deque = 0\n",
    "        self.adapted_overall_sum_of_td_errors = 0\n",
    "\n",
    "        # The degree to which prioritiztion is done (the closer to 1 the more)\n",
    "        self.alpha = hyperparameters['alpha_prioritized_replay']\n",
    "        # The degree to which correction for the bias is carried out \n",
    "        self.beta = hyperparameters['beta_prioritized_replay']\n",
    "        # Epsilon in the definition of p_{i}\n",
    "        self.incremental_td_error = hyperparameters['incremental_td_error']\n",
    "        # The size of the sample used to create a batch of experiences\n",
    "        self.batch_size = hyperparameters['batch_size']\n",
    "\n",
    "        # Indexes in the heap for which there was a change in the TD error\n",
    "        self.heap_indexes_to_update_td_error_for = None\n",
    "\n",
    "        # Set the device to cpu\n",
    "        self.device = 'cpu'\n",
    "\n",
    "    def initialize_td_errors_array(self):\n",
    "        return np.zeros(self.max_size)\n",
    "\n",
    "    def add_experience(self, td_error, state, action, next_state, reward):\n",
    "        # Get the TD error of the current element in the buffer (to be replaced) and\n",
    "        # remove it from the overall sum of TD errors and add the TD error of the \n",
    "        # new experience.\n",
    "        self.update_overall_sum(td_error, self.deque[self.deque_index_to_overwrite_next].key)\n",
    "        self.update_deque_and_deque_td_errors(td_error, state, action, next_state, reward)\n",
    "        self.update_heap_and_heap_index_to_overwrite()\n",
    "        self.update_number_experiences_in_deque()\n",
    "        self.update_deque_index_to_overwrite_next()\n",
    "\n",
    "    def update_overall_sum(self, new_td_error, old_td_error):\n",
    "        '''Updates the overall sum of td values in the buffer'''\n",
    "        self.adapted_overall_sum_of_td_errors += new_td_error - old_td_error\n",
    "\n",
    "    def update_deque_and_deque_td_errors(self, td_error, state, action, next_state, reward):\n",
    "        self.deques_td_errors[self.deque_index_to_overwrite_next] = td_error\n",
    "        self.add_element_to_deque(td_error, Transition(state, action, next_state, reward))\n",
    "    \n",
    "    def add_element_to_deque(self, new_key, new_value):\n",
    "        self.update_deque_node_key_and_value(self.deque_index_to_overwrite_next, new_key, new_value)\n",
    "    \n",
    "    def update_heap_and_heap_index_to_overwrite(self):\n",
    "        if not self.reached_max_capacity:\n",
    "            # The heap_index is always one step ahead of the deque index (under the preceding if condition) \n",
    "            self.update_heap_element(self.heap_index_to_overwrite_next, self.deque[self.deque_index_to_overwrite_next])\n",
    "            # Add a new attribute to the node at deque_index_to_overwrite_next that shows its index in heap  \n",
    "            self.deque[self.deque_index_to_overwrite_next].heap_index = self.heap_index_to_overwrite_next\n",
    "            # Increment by one the heap index (1 (start) -> 2 -> 3 ...)\n",
    "            self.update_heap_index_to_overwrite_next()\n",
    "        # When the capacity is full, the element located at index 0 of deque\n",
    "        # will be modified with the data of the new experience. We do not\n",
    "        # create a new instance of the Node object, but we only modify the \n",
    "        # key and the value of the already existing instance. This will also \n",
    "        # modify the data of the corresponding node in the heap. Therefore, we \n",
    "        # have the heap index (assigned before) and we need to reorganize the\n",
    "        # heap using that. \n",
    "        heap_index_change = self.deque[self.deque_index_to_overwrite_next].heap_index\n",
    "        self.reorganize_heap(heap_index_change)\n",
    "    \n",
    "    def update_number_experiences_in_deque(self):\n",
    "        '''Increments the attribute number_experiences_in_deque by one\n",
    "           if we have not reached the end of the queue. \n",
    "        '''\n",
    "        if not self.reached_max_capacity:\n",
    "            self.number_experiences_in_deque += 1\n",
    "\n",
    "    def update_heap_index_to_overwrite_next(self):\n",
    "        self.heap_index_to_overwrite_next += 1\n",
    "    \n",
    "    def swap_heap_elements(self, index1, index2):\n",
    "        self.heap[index1], self.heap[index2] = self.heap[index2], self.heap[index1]\n",
    "        # Update the heap index for the nodes (will be also reflected in their copies in deque)\n",
    "        self.heap[index1].heap_index = index1\n",
    "        self.heap[index2].heap_index = index2\n",
    "\n",
    "    def sample(self):\n",
    "        # Get a sample from the experiences in the buffer and return the index of the selected ones\n",
    "        experiences, deque_sample_indexes = self.pick_experiences_based_on_proportional_td_error()\n",
    "        transitions = self.separate_out_data_types(experiences)\n",
    "        self.deque_sample_indexes_to_update_td_error_for = deque_sample_indexes\n",
    "        importance_sampling_weights = self.calculate_importance_sampling_weights(experiences)\n",
    "        return transitions, importance_sampling_weights\n",
    "    \n",
    "    def pick_experiences_based_on_proportional_td_error(self):\n",
    "        # P(i) ~ p_{i}/sum(p_{j})\n",
    "        probabilities = self.deques_td_errors / self.give_adapted_sum_of_td_errors()\n",
    "        deque_sample_indexes = np.random.choice(range(len(self.deques_td_errors)), size=self.batch_size, replace=False, p=probabilities)\n",
    "        experiences = self.deque[deque_sample_indexes]\n",
    "        # Experiences sampled and their indexes in the deque\n",
    "        return experiences, deque_sample_indexes\n",
    "\n",
    "    def separate_out_data_types(self, experiences):\n",
    "        transitions = [e.value for e in experiences] \n",
    "        return transitions\n",
    "    \n",
    "    def calculate_importance_sampling_weights(self, experiences):\n",
    "        '''Calculate the w_{i} for i in the sampled batch'''\n",
    "        td_errors = [experience.key for experience in experiences]\n",
    "        importance_sampling_weights = [((1 / self.number_experiences_in_deque) * (self.give_adapted_sum_of_td_errors() / td_error)) ** self.beta for td_error in td_errors]\n",
    "        sample_max_importance_weight = max(importance_sampling_weights)\n",
    "        importance_sampling_weights = [is_weight / sample_max_importance_weight for is_weight in importance_sampling_weights]\n",
    "        # A 1-d tensor that will be multiplied to the 1-d delta tensor\n",
    "        importance_sampling_weights = torch.tensor(importance_sampling_weights).float().to(self.device)\n",
    "        return importance_sampling_weights\n",
    "\n",
    "    def update_td_errors(self, td_errors):\n",
    "        '''Updates the TD error of the replayed experiences.'''\n",
    "        for raw_td_error, deque_index in zip(td_errors, self.deque_sample_indexes_to_update_td_error_for):\n",
    "            td_error =  (abs(raw_td_error) + self.incremental_td_error) ** self.alpha\n",
    "            corresponding_heap_index = self.deque[deque_index].heap_index\n",
    "            self.update_overall_sum(td_error, self.heap[corresponding_heap_index].key)\n",
    "            self.heap[corresponding_heap_index].key = td_error\n",
    "            self.reorganize_heap(corresponding_heap_index)\n",
    "            self.deques_td_errors[deque_index] = td_error\n",
    "\n",
    "    def give_max_td_error(self):\n",
    "        return self.give_max_key()\n",
    "\n",
    "    def give_adapted_sum_of_td_errors(self):\n",
    "        return self.adapted_overall_sum_of_td_errors\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.number_experiences_in_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 256,\n",
    "    'buffer_size': 40000,\n",
    "    'alpha_prioritized_replay': 0.6,\n",
    "    'beta_prioritized_replay': 0.1,\n",
    "    'incremental_td_error': 1e-8,\n",
    "}\n",
    "\n",
    "# Prioritized replay memory\n",
    "prm = PrioritizedReplayBuffer(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    '''\n",
    "    Double DQN with Prioritized sampling (Schaul et al, 2016)\n",
    "    '''\n",
    "    # No update if the buffer is not adequately populated\n",
    "    if len(prm) < prm.batch_size:\n",
    "        return\n",
    "    \n",
    "    # Sampled transitions and the importance sampling for each\n",
    "    transitions, importance_sampling_weights = prm.sample()\n",
    "    # Create the batch from the sampled transitions\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    # A boolean tensor (true if the next state is not None and false, otherwise)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # Concat only next states that are not None (with a non zero Q)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                 if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Get the action values (expected Q) for each state and the action chosen\n",
    "    Q_expected = policy_net(state_batch).gather(1, action_batch)\n",
    "    # Get the Q for the next state\n",
    "    next_state_values = torch.zeros(prm.batch_size, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get the action for the next state whose expected value\n",
    "        # evaluated by the policy network is maximum (argmax Q(next_state, action; θ))\n",
    "        next_state_actions = policy_net(non_final_next_states).max(1).indices.view(-1, 1)\n",
    "        # Get the value of the action (selected above) using\n",
    "        # the target network (Q(next_state, action; θ'))\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states) \\\n",
    "            .gather(1, next_state_actions).squeeze(1)\n",
    "    \n",
    "    # Q-targets\n",
    "    Q_targets = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    # New TD-errors for the sampled transitions\n",
    "    td_errors =  Q_targets.detach().numpy() - Q_expected.squeeze(1).detach().numpy()\n",
    "    # Update the td errors in the memory for the transitions in the batch\n",
    "    prm.update_td_errors(td_errors)\n",
    "    \n",
    "    # Huber loss (element-wise)\n",
    "    loss = F.smooth_l1_loss(Q_expected, Q_targets.unsqueeze(1))\n",
    "    loss = loss * importance_sampling_weights\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Apply the gradient clipping\n",
    "    nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    # Update the parameters of the policy network\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 600\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for _ in count():\n",
    "        # Get the action using the current Q network\n",
    "        action = select_action(state)\n",
    "        # Get the next state, reward, status of the next state, ...\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "        # If the next state is a terminal state, then set it to None\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            # A row vector of shape (1, 4)\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        # Add the experience to the buffer \n",
    "        if len(prm) < prm.batch_size:\n",
    "            prm.add_experience(prm.incremental_td_error ** prm.alpha, state, action, next_state, reward)\n",
    "        else:\n",
    "            prm.add_experience(prm.give_max_td_error(), state, action, next_state, reward)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Next replay iteration and optimization\n",
    "        optimize_model()\n",
    "        \n",
    "        # Copy the parameters of the policy network to the target network\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key] * TAU + target_net_state_dict[key] * (1 - TAU)\n",
    "        # Update the parameters of the target network\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        if done:\n",
    "            # End of the episode\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
